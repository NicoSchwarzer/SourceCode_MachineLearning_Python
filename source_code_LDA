# -*- coding: utf-8 -*-
"""
Created on Sat Jun  5 10:44:13 2021

@author: Nico
"""


###########
## Ass 4 ##
###########

import numpy as np
import pandas as pd 
import os 
import scipy
from scipy.optimize import minimize
import matplotlib.pyplot as plt
import seaborn as sns


# setting WD
os.chdir("C:/Users/Nico/Documents/TÃ¼bingen/2. Sem/Stat ML/EX/EX5")


# plotting via 2 main dimensions ?? 




#######
## a ## 
#######


def LDAFeature(X,Y):
        
    # dividing into positive and negative classes 
    
    # computing within and between var
    X = Xtrain
    Y = Ytrain
    
    # finding means per class 
    La = np.ones([X.shape[0], X.shape[1] + 1 ])
    La[:,0:X.shape[1]] = X
    La[:,-1] = Y
    
    range_dim = X.shape[1]

    
    list_vals_Y = list(np.unique(Y))
    
    
######## 
## CB ##     
########
    
        
    # means per feature in general 
    feature_means = np.ones([range_dim,1])
    
    for i in range(range_dim):
        feature_m = np.mean(X[:,i])
        feature_means[i,] = feature_m
        
        
    # CB matrix instentiating 
    CB = np.zeros([range_dim,range_dim])
    
    for a in list_vals_Y:
        la_x = La[La[:,-1] == a]
        means = np.ones([range_dim,1])
        means_minus_means = np.ones([range_dim,1])
        
        
        for i in range(range_dim):
            mean = np.mean(la_x[:,i])
            means[i,] = mean
        
        
        means_minus_means = means - feature_means
        CB_a = np.matmul(means_minus_means, means_minus_means.T)
        CB  = CB + CB_a
         
########        
## CW ##
########
        
    # CW matrix instentiating 
    CW = np.zeros([range_dim,range_dim])

    # means per class and x values minus these means 
    for a in list_vals_Y:
        
        la_x = La[La[:,-1] == a]
        
        # mmeans per class
        means_class = np.ones([range_dim,1])
        
        for i in range(range_dim):
            means_class[i,] = np.mean(la_x[:,i])
        
        # for each subclass - sum of: Xi - means 
        xi_minus_means = np.zeros([range_dim,range_dim])
        
        for i in range(la_x.shape[0]):
            xi = la_x[i,range_dim].T
            diff = np.matmul( (xi - means_class), (xi - means_class).T)
            xi_minus_means = xi_minus_means + diff 
            
        CW_a = xi_minus_means
        CW = CW + CW_a
        
 
# final step #      - evtl noch etwas machen     

    D, V = scipy.linalg.eig(CB, CW)
     

    s = np.argsort(D)
    eigv_1 = np.where(s == max(s))[0][0]
    eigv_2 = np.where(s == (max(s)-1) )[0][0]
    
    
    vectors = W[:,[eigv_1, eigv_2]]
    
    
    
    
    Z = np.matmul(X,vectors)
    #Z.shape

    
    
    return Z, vectors
 


#######
## c ##
#######


#For different dimensions of the feature space, d = 5; 30; 60; 75, 
#sample 25 points from each class, 
#X=numpy.random.randn(25,d) and generate the embedding with LDA.


d = [5,30,60,75]




#######
## d ##
#######

## data ##

a = np.load("digits389.npy", allow_pickle=True).item()

result = a.items()
data = list(result)
data2 = np.array(data, dtype=object)

# finally retrieving data: 
    
# Xtest
print("the values for " + str(data2[0,0]) + " are stored in data2[0,1].")
Xtest =  data2[0,1]

#Xtrain
print("the values for " + str(data2[1,0]) + " are stored in data2[1,1].")
Xtrain = data2[1,1]

#Yest 
print("the values for " + str(data2[2,0]) + " are stored in data2[2,1].")
Ytest = data2[2,1]

#Ytrain
print("the values for " + str(data2[3,0]) + " are stored in data2[3,1].")
Ytrain = data2[3,1]


###

# obtaining the eigenvectors
[Z,W] = LDAFeature(Xtrain,Ytrain)

### training 

Z_train = np.ones([Z.shape[0], Z.shape[1] + 1 ])
Z_train[:,0:Z.shape[1]] = Z
Z_train[:,-1] = Ytrain


## plotting ## 
Z_train = pd.DataFrame(Z_train)

plt.figure()
plt.scatter(Z_train[0], Z_train[1], c = Z_train[2])

### test set 

# multiplying by X test by W
reduced_Z = np.matmul(Xtest, W)


Z_test = np.ones([reduced_Z.shape[0], reduced_Z.shape[1] + 1 ])
Z_test[:,0:reduced_Z.shape[1]] = reduced_Z
Z_test[:,-1] = Ytest


## plotting ## 
Z_test = pd.DataFrame(Z_test)

plt.figure()
plt.scatter(Z_test[0], Z_test[1], c = Z_test[2])







