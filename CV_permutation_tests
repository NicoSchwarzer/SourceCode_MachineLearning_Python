# -*- coding: utf-8 -*-
"""
Created on Sun Jun 27 13:05:59 2021

@author: Nico
"""

##################
## Assignment 8 ##
##################


import os
import numpy as np
import pandas as pd 
import os 
import scipy
from scipy.optimize import minimize
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.ticker import LinearLocator
import itertools



## loading in data ##

direct = "C:/Users/Nico/Documents/Uni/2. Sem/Stat ML/EX/EX8"
                        
os.chdir(direct)

a = np.load("DataFeatSel.npy", allow_pickle=True).item()


result = a.items()
data = list(result)
data2 = np.array(data, dtype=object)

data2.shape

# finally retrieving data: 
    
# Xtrain
print("the values for " + str(data2[0,0]) + " are stored in data2[0,1].")
Xtrain =  data2[0,1]

#Ytrain
print("the values for " + str(data2[1,0]) + " are stored in data2[1,1].")
Ytrain = data2[1,1]

#Ytest
print("the values for " + str(data2[2,0]) + " are stored in data2[2,1].")
Ytest = data2[2,1]

#Xtest
print("the values for " + str(data2[3,0]) + " are stored in data2[3,1].")
Xtest = data2[3,1]




## all possible combinations   
# each row -> one combination 

all_combi = np.array(list(itertools.product(range(2), repeat=15)))


## defining function for losses

def OLS_and_error(Xt, Yt, X_val,  Y_val):


    # computing weights
    c1 = np.linalg.inv(np.matmul(Xt.T, Xt))
    c2 = np.matmul(Xt.T, Yt)
    
    w = np.matmul(c1, c2)
    
    # computing prediction
    pred = np.sign(np.matmul(X_val, w))
    
    # loss
    all_loss = np.absolute(Y_val - pred)
    loss = 0.5 * 1/ X_val.shape[0]* np.sum(all_loss)
    
    return loss



## function for 5-fold CV:
    
    

def five_fold_CV(Xtrain, combi):
    
    a = 0
    b = 7
    i = 0
    
    losses = np.ones(5)
    
   # dropping featureds
    dont_keep = np.where(combi == 0)[0]
    
    # creating new X, maybe adding intercept
    X2 =  np.delete(Xtrain, dont_keep, 1)
  
    while a < 40:
        
        X_val = X2[a:b,:]
        Y_val = Ytrain[a:b,:]
        
        lsp = list(np.linspace(a,b,8))
        lsp = [int(item) for item in lsp]
        
        Xt =  np.delete(X2, lsp, 0)
        Yt = np.delete(Ytrain, lsp, 0)
        
        
        loss = OLS_and_error(Xt, Yt, X_val,  Y_val)
        
        losses[i] = loss
    
        a = a + 8
        b = b + 8
        i = i + 1       
    

    avg_loss = np.average(losses)
    
    return avg_loss


     
## computing all CV losses

cv_losses = np.ones(len(all_combi))


for i in range(len(all_combi)):

    # 
    combi = all_combi[i,:]
    loss_cv = five_fold_CV(Xtrain,combi)
    
    cv_losses[i] = loss_cv
    


# checking where the mininum was found 

min_value = min(cv_losses)

min_index = np.argmin(cv_losses)

best_combi = all_combi[min_index,:]

#
feats = np.where(best_combi == 1)[0]

# keeping the features No 4, No 10 and No 11!

print("We shall keep the features ",  str(feats[0]+1),", ",str(feats[1]+1)," and ",str(feats[2]+1),"!")


## learning classifier!

dont_keep = np.where(best_combi == 0)[0]
X_best =  np.delete(Xtrain, dont_keep, 1)

c1 = np.linalg.inv(np.matmul(X_best.T, X_best))
c2 = np.matmul(X_best.T, Ytrain)
w = np.matmul(c1, c2)


# predicting and test loss 

X_test_best =  np.delete(Xtest, dont_keep, 1)
pred_test = np.sign(np.matmul(X_test_best, w))
all_loss = np.absolute(Ytest - pred_test)
loss_test = 0.5 * 1/ X_test_best.shape[0]* np.sum(all_loss)


print("The new test loss is: ", str(loss_test), "!")


# different because of the extremely small size of CV sets (only 8)
# maybe 



##########
## NO 2 ##
##########

## weong without considering test set ??

# HO: independence of X(i) and Y(i)

# if data were truly independent -> best_error (always) = 0.5 !!
# p-value grows as t > t original -> work only for for 0 centered t-stat under Ho




# independence Xi, Yi 
# i.e. of those Xi, which are in best subset 

# test stat: best cv error 

# if data were truly independent -> best_error (always) = 0.5 !!
    


### error original ###


Xtrain_smaller = Xtrain[:,[0,1,2,3,4,5]]
Xtest_smaller = Xtest[:,[0,1,2,3,4,5]]


all_combi_smaller = np.array(list(itertools.product(range(2), repeat= 6)))
cv_losses_smaller = np.ones(len(all_combi_smaller))


for i in range(len(all_combi_smaller)):

    # 
    combi = all_combi_smaller[i,:]
    loss_cv = five_fold_CV(Xtrain_smaller,combi)
    
    cv_losses_smaller[i] = loss_cv
                     

# getting best feature subset and computing respective weight vector


min_cv_error_original = min(cv_losses_smaller)

print("The original CV loss is ", min_cv_error_original, "!")


### other cv errors ###
np.random.seed(1)

cv_errors_perm = np.ones(1000)

a = 0


while a < 1000:
    
    # getting permutations    
    X_perm = np.random.permutation(Xtrain_smaller)
    
    cv_losses = np.ones(len(all_combi_smaller))

    # same cacultation as above 
    for i in range(len(all_combi_smaller)):

        
        combi_perm = all_combi_smaller[i,:]
        loss_cv_perm = five_fold_CV(X_perm,combi_perm)
    
        cv_losses[i] = loss_cv_perm
        
                     
    min_cv_error_perm = min(cv_losses)
        
    cv_errors_perm[a] =  min_cv_error_perm
        
    a = a + 1
     
    
    
# now have have the permutational distribution     
plt.hist(cv_errors_perm)


# 0.05% of that 
five_perc_percentile = np.percentile(cv_errors_perm, 0.05)
 
print("The 5_% quantile of the hereby obtained permutational distribution is ", five_perc_percentile, "!")


## doing the actual test

if min_cv_error_original > five_perc_percentile:
    print("We cannot reject H0 of independence with a 5% significance level")
    print("This suggests that the data are truly independet")
else:
    print("We can reject H0 of independence with a 5% significance level")
    
