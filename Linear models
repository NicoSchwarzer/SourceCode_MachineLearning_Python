# -*- coding: utf-8 -*-
"""
Created on Fri May 14 17:55:57 2021

@author: Nico
"""

############
### Ex 6 ###
############

import numpy as np
import pandas as pd 
import os 
import scipy
from scipy.optimize import minimize
import matplotlib.pyplot as plt
import seaborn as sns


#####
# a #
#####

## ordinary least squares


def LeastSquares(X, Y):
    
    #X: deisgn matrix n x d
    #Y: true values n x 1
    
    #output: weight of linear regression d x 1

    a = np.transpose(X)
    b = X
    c = np.matmul(a,b)
    
    d = np.matmul(X.T, Y)

    w = np.linalg.solve(c, d)
    w = np.expand_dims(w,1)

    si = X.shape[1]
    w = w.reshape(si,)
 
    return w


## least sqaures again, this time using the pseudo inverse to avoid singularity issues 

def LeastSquares_2(X, Y):
    
    #X: deisgn matrix n x d
    #Y: true values n x 1
    
    #output: weight of linear regression d x 1

    # using pseudo-inverse here  = (X'X)^-1 * X'
    pseudo_inv = np.linalg.pinv(X)
    
    w = np.matmul(pseudo_inv, Y)
    w = np.expand_dims(w,1)

    si = X.shape[1]
    w = w.reshape(si,)
 
    return w


    
  
# to check:
#LeastSquares(Xtrain, Ytrain)
#s1 = LeastSquares(b1, Ytrain)


## Ridge Regression 
       

def  RidgeRegression(X, Y, lmbd_reg):
    ''' solves linear regression with
    L1 Loss + L2 regularization

    X: deisgn matrix n x d
    Y: true values n x 1
    lmbd_reg: weight regularization

    output: weight of linear regression d x 1
    '''
    a = np.transpose(X)
    b = X
    c = np.matmul(a,b)
    d = np.matmul(X.T, Y)
    e = lmbd_reg * np.identity(X.shape[1])
    f = c + e
        
    w = np.linalg.solve(f, d)
    w = np.expand_dims(w,1)
    si = X.shape[1]
    w = w.reshape(si,)
 
    return w

# to check:    
#RidgeRegression(Xtrain, Ytrain, 30)
#s2 = RidgeRegression(b1, Ytrain, 30)


#####
# b #
#####

# d= 1 

def Basis(X,k):
    # making sure that X has the correct dimension
    
    dim_x = np.size(X, 0)
    X = X.reshape(dim_x)
    
    dim2 = 2*k + 1
    phi_mat = np.ones([dim_x, dim2])
    # first column
    phi_mat[:,0] = 1
    
    # other columns 
        
    for i in range(1, (k+1)):

        # use  cosine function for 2*l - 1:
        phi_mat[:,(2*i-1)] = np.cos(2*np.pi*i*X)
        
        # use sine function for 2*l:
        phi_mat[:,(2*i)] = np.sin(2*np.pi*i*X)
            
    # finally:
    
    return phi_mat


#####
# c #
#####

# setting WD
os.chdir("C:/Users/Nico/Documents/TÃ¼bingen/2. Sem/Stat ML/EX/EX3")

# using: numpy.load(<file name>).item(),
a = np.load("onedim_data.npy", allow_pickle=True).item()

result = a.items()
data = list(result)
data2 = np.array(data, dtype=object)

#data2.size

# finally retrieving data: 
    
# Xtest
print("the values for " + str(data2[0,0]) + " are stored in data2[0,1].")
Xtest =  data2[0,1]

#Xtrain
print("the values for " + str(data2[1,0]) + " are stored in data2[1,1].")
Xtrain = data2[1,1]

#Ytrain
print("the values for " + str(data2[2,0]) + " are stored in data2[2,1].")
Ytrain = data2[2,1]

#Ytest
print("the values for " + str(data2[3,0]) + " are stored in data2[3,1].")
Ytest = data2[3,1]


## plotting - using histograms

# since the function will only be trained
# on the training data, only considering Xtrain 

plt.figure()
plt.hist(Xtrain, bins = 80)
plt.title("distribution of dependent variables ")
plt.savefig("plot_ex_6_c")

# rather uniform - L2 is plausible!



### plot of resulting functions  fk ###

K = [1, 2, 3, 5, 10, 15, 20]
lmbd = 30 

xx = np.linspace(0,1,1000)

# fks - 1000 * 7
fks = np.zeros([1000,7])


for k in K:
    #  first - getting w(k) using ridge regressio on training data
    X_mat = Basis(Xtrain, k)
    w = RidgeRegression(X_mat, Ytrain, 30)
    
    # getting index of each k
    index = K.index(k)
    
    # getting phi(x) for 0<x<1!
    phi_x = Basis(xx, k)
    
    for a in range(phi_x.shape[0]):
        x_a = phi_x[a, :]
        fk_xa = np.inner(x_a, w)
        fks[a, index] = fk_xa ## range ok K !!!
        

fks = pd.DataFrame(fks)
fks.columns = ["k = 1", "k = 2", "k = 3", "k = 5", "k = 10", "k = 15", "k = 20"]

# plotting the fks

plt.figure()
plt.plot(xx, fks["k = 1"])
plt.plot(xx, fks["k = 2"])
plt.plot(xx, fks["k = 3"])
plt.plot(xx, fks["k = 5"])
plt.plot(xx, fks["k = 10"])
plt.plot(xx, fks["k = 15"])
plt.plot(xx, fks["k = 20"])
plt.xlabel("x", size=14)
plt.ylabel("function value ", size=14)
plt.title("Fk(x) for different k (Ex. 6)", size = 14)  
plt.legend(["k = 1", "k = 2", "k = 3", "k = 5", "k = 10", "k = 15", "k = 20"], loc = "lower left")
plt.show()
plt.savefig("plot_ex_6_fx_different_k")


# the higher k , the less smooth is the function behaaviour. 


### plotting the losses ###


## first for ridge regression ##


losses_r_train = []
losses_r_test = []
    
# losses of training 
for k in K:
    X_mat = Basis(Xtrain, k)
    w = RidgeRegression(X_mat, Ytrain, 30)
    
    # for training set 
    ypred_1 = np.matmul(X_mat, w).reshape(1000,1)
    loss_1 = (1 / X_mat.shape[0] ) *  (np.matmul(   ((ypred_1-Ytrain).T),(ypred_1-Ytrain) )   ).sum()
    losses_r_train.append(loss_1)
    
    # for test set 
    X_mat2 = Basis(Xtest, k)
    ypred_2 = np.matmul(X_mat2, w).reshape(1000,1)
    loss_2 = (1 / X_mat2.shape[0] ) *  (np.matmul(   ((ypred_2-Ytest).T),(ypred_2-Ytest) )   ).sum()
    losses_r_test.append(loss_2)
    


# plotting both losses 
plt.figure()
plt.plot(K, losses_r_train, color = "blue")
plt.plot(K, losses_r_test, color = "red",  linestyle = '--')
plt.xlabel("K in fourier basis", size=14)
plt.ylabel("L2-Loss", size=14)
plt.title("Loss for ridge regression (Ex. 6)")  
plt.legend(["Loss for training set", "Loss for test set"], loc ="lower left")
plt.show()
plt.savefig("plot_ex_6_ridge_lossses")


## secondly for no regularization ##

losses_r_train_2 = []
losses_r_test_2 = []
    
# losses of training 
for k in K:
    X_mat = Basis(Xtrain, k)
    w = LeastSquares(X_mat, Ytrain)  # or _2
    
    # for training set 
    ypred_1 = np.matmul(X_mat, w).reshape(1000,1)
    loss_1 = (1 / X_mat.shape[0] ) *  (np.matmul(   ((ypred_1-Ytrain).T),(ypred_1-Ytrain) )   ).sum()
    losses_r_train_2.append(loss_1)
    
    # for test set 
    X_mat2 = Basis(Xtest, k)
    ypred_2 = np.matmul(X_mat2, w).reshape(1000,1)
    loss_2 = (1 / X_mat2.shape[0] ) *  (np.matmul(   ((ypred_2-Ytest).T),(ypred_2-Ytest) )   ).sum()
    losses_r_test_2.append(loss_2)
    



# plotting both losses 
plt.figure()
plt.plot(K, losses_r_train_2, color = "blue")
plt.plot(K, losses_r_test_2, color = "red",  linestyle = '--')
plt.xlabel("K in fourier basis", size=14)
plt.ylabel("L2-Loss", size=14)
plt.title("Loss for Least Sqaures (Ex. 6)")  
plt.legend(["Loss for training set", "Loss for test set"], loc ="lower left")
plt.show()
plt.savefig("plot_ex_6_OLS_lossses")


## -- > almost no change. The  OLS loss for the test set is somwhat higher!




#########
### d ###
#########


### d2 - fourier basis normailzed 


def FourierBasisNormalized(X,k):
    # making sure that X has the correct dimension
    
    dim_x = np.size(X, 0)
    X = X.reshape(dim_x)
    
    dim2 = 2*k + 1
    phi_mat_norm = np.ones([dim_x, dim2])
    # first column
    phi_mat_norm[:,0] = 1
    
    # other columns 
        
        
    for i in range(1, (k+1)):

        # use  cosine function for 2*l - 1:
        phi_mat_norm[:,(2*i-1)] = np.cos(2*np.pi*i*X) *  0.5 * 1 / np.sqrt(np.pi**2 * i**3) 
        
        # use sine function for 2*l:
        phi_mat_norm[:,(2*i)] = np.sin(2*np.pi*i*X) *  0.5 * 1 / np.sqrt(np.pi**2 * i**3)
        
    # finally:
    
    return phi_mat_norm



## again: 
### plot of resulting functions  fk ###

K = [1, 2, 3, 5, 10, 15, 20]
lmbd = 30 

xx = np.linspace(0,1,1000)

# fks - 1000 * 7
fks_2 = np.zeros([1000,7])


for k in K:
    #  first - getting w(k) using ridge regressio on training data
    X_mat = FourierBasisNormalized(Xtrain, k)
    w = RidgeRegression(X_mat, Ytrain, 30)
    
    # getting index of each k
    index = K.index(k)
    
    # getting phi(x) for 0<x<1!
    phi_x = FourierBasisNormalized(xx, k)
    
    for a in range(phi_x.shape[0]):
        x_a = phi_x[a, :]
        fk_xa = np.inner(x_a, w)
        fks_2[a, index] = fk_xa ## range ok K !!!
        

fks_2 = pd.DataFrame(fks_2)
fks_2.columns = ["k = 1", "k = 2", "k = 3", "k = 5", "k = 10", "k = 15", "k = 20"]

# plotting the fks

plt.figure()
plt.plot(xx, fks_2["k = 1"])
plt.plot(xx, fks_2["k = 2"])
plt.plot(xx, fks_2["k = 3"])
plt.plot(xx, fks_2["k = 5"])
plt.plot(xx, fks_2["k = 10"])
plt.plot(xx, fks_2["k = 15"])
plt.plot(xx, fks_2["k = 20"])
plt.xlabel("x", size=14)
plt.ylabel("function value ", size=14)
plt.title("Fk(x) for different k - normalized (Ex. 6)", size = 14)  
plt.legend(["k = 1", "k = 2", "k = 3", "k = 5", "k = 10", "k = 15", "k = 20"], loc = "lower left")
plt.show()
plt.savefig("plot_ex_6_fx_different_k_norm")



# much smoother!


## again: plotting the losses

## first for ridge regression ##


losses_r_train = []
losses_r_test = []

losses_r_train_norm = []
losses_r_test_norm = []

    
# losses of training 
for k in K:
    X_mat = Basis(Xtrain, k)
    w = RidgeRegression(X_mat, Ytrain, 30)
    
    ## non - normalized! ##
    
    # for training set 
    ypred_1 = np.matmul(X_mat, w).reshape(1000,1)
    loss_1 = (1 / X_mat.shape[0] ) *  (np.matmul(   ((ypred_1-Ytrain).T),(ypred_1-Ytrain) )   ).sum()
    losses_r_train.append(loss_1)
    
    # for test set 
    X_mat2 = Basis(Xtest, k)
    ypred_2 = np.matmul(X_mat2, w).reshape(1000,1)
    loss_2 = (1 / X_mat2.shape[0] ) *  (np.matmul(   ((ypred_2-Ytest).T),(ypred_2-Ytest) )   ).sum()
    losses_r_test.append(loss_2)
    
    
    ## normalized ! ## 
    
    X_mat_norm = FourierBasisNormalized(Xtrain, k)
    w_norm = RidgeRegression(X_mat_norm, Ytrain, 0.5)
    
    
    # for training set 
    ypred_3 = np.matmul(X_mat_norm, w_norm).reshape(1000,1)
    loss_3 = (1 / X_mat.shape[0] ) *  (np.matmul(   ((ypred_3-Ytrain).T),(ypred_3-Ytrain) )   ).sum()
    losses_r_train_norm.append(loss_3)
    
    # for test set 
    X_mat_norm_2 = FourierBasisNormalized(Xtest, k)
    ypred_4 = np.matmul(X_mat_norm_2, w_norm).reshape(1000,1)
    loss_4 = (1 / X_mat_norm_2.shape[0] ) *  (np.matmul(   ((ypred_4-Ytest).T),(ypred_4-Ytest) )   ).sum()
    losses_r_test_norm.append(loss_4)
    
    


# plotting all 4 losses 
plt.figure()
# using non-normalized psi
plt.plot(K, losses_r_train, color = "blue")
plt.plot(K, losses_r_test, color = "red")
# normalized psi 
plt.plot(K, losses_r_train, color = "blue",  linestyle = '--')
plt.plot(K, losses_r_test, color = "red",  linestyle = '--')
plt.xlabel("K in fourier basis", size=14)
plt.ylabel("L2-Loss", size=14)
plt.title("Loss for ridge regression - also considered normalized psis (Ex. 6)")  
plt.legend(["Loss for training set", "Loss for test set", "Loss for training set - normalized", "Loss for test set - normalized"], loc ="upper right")
plt.show()
plt.savefig("plot_ex_6_ridge_lossses_also_norm")


# no difference visible at all!


## secondly for no regularization ##

losses_r_train_2 = []
losses_r_test_2 = []

losses_r_train_norm_2 = []
losses_r_test_norm_2 = []

    
# losses of training 
for k in K:
    X_mat = Basis(Xtrain, k)
    w = LeastSquares(X_mat, Ytrain)  # or _2
    
    # non-normalized # 
    
    # for training set 
    ypred_1 = np.matmul(X_mat, w).reshape(1000,1)
    loss_1 = (1 / X_mat.shape[0] ) *  (np.matmul(   ((ypred_1-Ytrain).T),(ypred_1-Ytrain) )   ).sum()
    losses_r_train_2.append(loss_1)
    
    # for test set 
    X_mat2 = Basis(Xtest, k)
    ypred_2 = np.matmul(X_mat2, w).reshape(1000,1)
    loss_2 = (1 / X_mat2.shape[0] ) *  (np.matmul(   ((ypred_2-Ytest).T),(ypred_2-Ytest) )   ).sum()
    losses_r_test_2.append(loss_2)
    
    
    # normalized
    
    X_mat_norm = FourierBasisNormalized(Xtrain, k)
    w_norm = LeastSquares(X_mat_norm, Ytrain)
    
    
    ypred_3 = np.matmul(X_mat_norm, w).reshape(1000,1)
    loss_3 = (1 / X_mat.shape[0] ) *  (np.matmul(   ((ypred_3-Ytrain).T),(ypred_3-Ytrain) )   ).sum()
    losses_r_train_norm_2.append(loss_3)
    
    # for test set 
    X_mat_norm_2 = FourierBasisNormalized(Xtest, k)
    ypred_3 = np.matmul(X_mat_norm_2, w_norm).reshape(1000,1)
    loss_4 = (1 / X_mat2.shape[0] ) *  (np.matmul(   ((ypred_3-Ytest).T),(ypred_3-Ytest) )   ).sum()
    losses_r_test_norm_2.append(loss_4)
    




# plotting all 4 losses 
plt.figure()
# using non-normalized psi
plt.plot(K, losses_r_train_2, color = "blue")
plt.plot(K, losses_r_test_2, color = "red")
# normalized psi 
plt.plot(K, losses_r_train_2, color = "blue",  linestyle = '--')
plt.plot(K, losses_r_test_2, color = "red",  linestyle = '--')
plt.xlabel("K in fourier basis", size=14)
plt.ylabel("L2-Loss", size=14)
plt.title("Loss for OLS regression - also considered normalized psis (Ex. 6)")  
plt.legend(["Loss for training set", "Loss for test set", "Loss for training set - normalized", "Loss for test set - normalized"], loc ="upper right")
plt.show()
plt.savefig("plot_ex_6_OLS_lossses_also_norm")


## again, test loss is somwhat higher! 


